{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Question-Answering Agent Tutorial\n",
    "\n",
    "### Overview\n",
    "\n",
    "This tutorial introduces a basic Question-Answering (QA) agent using LangChain and OpenAI's language model. The agent is designed to understand user queries and provide relevant, concise answers.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In the era of AI-driven interactions, creating a simple QA agent serves as a fundamental stepping stone towards more complex AI systems. This project aims to:\n",
    "\n",
    "Demonstrate the basics of AI-driven question-answering\n",
    "Introduce key concepts in building AI agents\n",
    "Provide a foundation for more advanced agent architectures\n",
    "Key Components\n",
    "\n",
    "Language Model: Utilizes OpenAI's GPT model for natural language understanding and generation.\n",
    "Prompt Template: Defines the structure and context for the agent's responses.\n",
    "LLMChain: Combines the language model and prompt template for streamlined processing.\n",
    "Method Details\n",
    "\n",
    "#### 1. Setup and Initialization\n",
    "\n",
    "Import necessary libraries (LangChain, dotenv)\n",
    "Load environment variables for API key management\n",
    "Initialize the OpenAI language model\n",
    "\n",
    "#### 2. Defining the Prompt Template\n",
    "\n",
    "Create a template that instructs the AI on its role and response format\n",
    "Use the PromptTemplate class to structure the input\n",
    "\n",
    "#### 3. Creating the LLMChain\n",
    "\n",
    "Combine the language model and prompt template into an LLMChain\n",
    "This chain manages the flow from user input to AI response\n",
    "\n",
    "#### 4. Implementing the Question-Answering Function\n",
    "\n",
    "Define a function that takes a user question as input\n",
    "Use the LLMChain to process the question and generate an answer\n",
    "\n",
    "#### 5. User Interaction\n",
    "\n",
    "In a Jupyter notebook environment, provide cells for:\n",
    "Example usage with a predefined question\n",
    "Interactive input for user questions\n",
    "Conclusion\n",
    "\n",
    "This Simple Question-Answering Agent serves as an entry point into the world of AI agents. By understanding and implementing this basic model, you've laid the groundwork for more sophisticated systems. Future enhancements could include:\n",
    "\n",
    "Adding memory to maintain context across multiple questions\n",
    "Integrating external knowledge bases for more informed responses\n",
    "Implementing more complex decision-making processes\n",
    "As you progress through more advanced tutorials in this repository, you'll build upon these fundamental concepts to create increasingly capable and intelligent AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o\",\n",
    "    max_tokens=1000,\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI assistant. Your task is to answer the user's question to the best of your ability.\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Please provide a clear and concise answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define get_answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    \"\"\"\n",
    "    Get an answer to the given question using the QA chain.\n",
    "    \"\"\"\n",
    "    input_variables = {\"question\": question}\n",
    "    response = qa_chain.invoke(input_variables).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive cell for user questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: As an artificial intelligence, I don't have a date of birth. I was created by OpenAI and my knowledge is based on data up to September 2021, with updates and improvements made over time. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Enter your question: \")\n",
    "user_answer = get_answer(user_question)\n",
    "print(f\"Answer: {user_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
