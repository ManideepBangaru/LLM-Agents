{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction to LangGraph </h1>\n",
    "\n",
    "LangGraph is a framework for creating applications using graph-based workflows. Each node represents a function or computational step, and edges define the flow between these nodes based on certain conditions.\n",
    "\n",
    "<h2> Key Features: </h2>\n",
    "\n",
    "State Management\n",
    "Flexible Routing\n",
    "Persistence\n",
    "Visualization\n",
    "\n",
    "Overview: Text Analysis Pipeline\n",
    "\n",
    "In this, let see the power of LangGraph by building a multi-step text analysis pipeline. Our use case will focus on processing a given text through three key stages:\n",
    "\n",
    "**Text Classification:** We'll categorize the input text into predefined categories (e.g., News, Blog, Research, or Other).\n",
    "**Entity Extraction:** We'll identify and extract key entities such as persons, organizations, and locations from the text.\n",
    "**Text Summarization:** Finally, we'll generate a concise summary of the input text.\n",
    "\n",
    "This pipeline showcases how LangGraph can be used to create a modular, extensible workflow for natural language processing tasks. By the end of this tutorial, you'll understand how to construct a graph-based application that can be easily modified or expanded for various text analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building the Text Processing Pipeline </h2>\n",
    "\n",
    "<h3> Define State and Initialize LLM </h3>\n",
    "\n",
    "Here we define the State class to hold our workflow data and initialize the ChatOpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    text : str\n",
    "    classification : str\n",
    "    entities : List[str]\n",
    "    summary : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Node Functions\n",
    "\n",
    "These functions define the operations performed at each node of our graph: classification, entity extraction and summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_node(state : State):\n",
    "    '''\n",
    "    Classify the text into one of the categories  : News, Blog, Research, Other\n",
    "    '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"text\"],\n",
    "        template = \"Classify the following text into one of the categories: News, Blog, Research, or Other.\\n\\nText:{text}\\n\\nCategory:\"        \n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    classification = llm.predict_messages([message]).content.strip()\n",
    "\n",
    "    return {\"classification\":classification}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_extraction_node(state: State):\n",
    "    ''' Extract all the entities (Person, Organization, Location) from the text '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Extract all the entities (Person, Organization, Location) from the following text. Provide the result as a comma-separated list.\\n\\nText:{text}\\n\\nEntities:\"\n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    entities = llm.predict_messages([message]).content.strip().split(\", \")\n",
    "    return {\"entities\": entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_node(state: State):\n",
    "    ''' Summarize the text in one short sentence '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Summarize the following text in one short sentence.\\n\\nText:{text}\\n\\nSummary:\"\n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    summary = llm.predict_messages([message]).content.strip()\n",
    "    return {\"summary\": summary}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
